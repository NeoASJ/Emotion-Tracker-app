<div align="center">

# ğŸ™ï¸ <span style="font-size:42px; font-weight:800;">Emotion Recognition from Speech</span>

### <span style="font-size:20px;">A Deep Learning System for Multilingual Emotion Detection using Audio Signal Processing & Cloud Deployment</span>

![Python](https://img.shields.io/badge/Python-3.9-blue?logo=python)
![TensorFlow](https://img.shields.io/badge/TensorFlow-GPU-orange?logo=tensorflow)
![Librosa](https://img.shields.io/badge/Audio-Librosa-lightgrey?logo=soundcloud)
![Docker](https://img.shields.io/badge/Containerized-Docker-blue?logo=docker)
![Cloud Run](https://img.shields.io/badge/Deployed-Google%20Cloud%20Run-4285F4?logo=google-cloud)
</div>

---

## ğŸ§­ <span style="font-size:26px;">Overview</span>

This project implements a **deep learningâ€“based emotion recognition system** that classifies human emotions from voice recordings.  
It integrates **audio preprocessing**, **neural model training**, and **cloud deployment** into a **production-grade pipeline**.

Trained on **three benchmark emotion datasets â€” RAVDESS, CREMA-D, and EMO-DB**, the models leverage **GPU acceleration (CUDA)** for efficient feature extraction and optimization.

The final Streamlit app is **Dockerized and deployed on Google Cloud Run**, supporting real-time inference.

---

âš¡ <span style="font-size:26px;">Key Highlights</span>

âœ… **High-Fidelity Audio:** Trained exclusively on `.wav` files  
ğŸ”„ **FFmpeg Pipeline:** Converts `.mp3` â†’ `.wav` seamlessly  
ğŸ› **Audio Features:** MFCCs, Chroma, Spectral Centroid, Zero Crossing Rate, RMS Energy  
ğŸ§  **Three Deep Models:**
- CNN â€” *RAVDESS*  
- BiLSTM â€” *CREMA-D*  
- CNN + BiLSTM Hybrid â€” *EMO-DB*
ğŸš€ **GPU-Accelerated** training (TensorFlow-GPU backend)  
ğŸŒ **Cloud Deployment** via Docker + GCP Cloud Run  
ğŸ’° **Budget Control:** Configured cost limits with usage alerts  

---

ğŸ§© <span style="font-size:26px;">Workflow Overview</span>

```
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚      Audio Dataset           â”‚
 â”‚     (.wav files)      â”‚
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
                â–¼
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚   FFmpeg Conversion          â”‚
 â”‚   (.mp3 â†’ .wav)              â”‚
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
                â–¼
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚   Feature Extraction          â”‚
 â”‚   (MFCC, Chroma, Spectral)    â”‚
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
                â–¼
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚   Model Training              â”‚
 â”‚   (CNN, BiLSTM, CNN+BiLSTM)  â”‚
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
                â–¼
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚   Model Evaluation &          â”‚
 â”‚   Confusion Matrix Generation â”‚
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
                â–¼
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚   Model Export                â”‚
 â”‚   (.h5 + .pkl files)          â”‚
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
                â–¼
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚   Docker Containerization     â”‚
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
                â–¼
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚   Google Cloud Run Deploymentâ”‚
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
                â–¼
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚   Streamlit Web App           â”‚
 â”‚   Real-Time Emotion Detection â”‚
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


---

ğŸ“š Dataset Details

| **Dataset** | **Samples** | **Emotions** | **Language** | **Notes** |
|--------------|--------------|---------------|---------------|-----------|
| ğŸ­ RAVDESS | 2880 | 8 (calm, happy, sad, angry, fearful, disgust, surprised, neutral) | English | Acted emotional speech |
| ğŸ™ CREMA-D | 7442 | 6 (angry, disgust, fear, happy, sad, neutral) | English | Crowd-acted dataset |
| ğŸ¤ EMO-DB | 583 | 7 (anger, boredom, disgust, fear, happy, neutral, sad) | German | High-quality emotional dataset |

Each dataset was processed and saved in `.pkl` format:

```python
ravdess_df.to_pickle("ravdess_features.pkl")
crema_df.to_pickle("crema_features.pkl")
emodb_df.to_pickle("emodb_features.pkl")
```

---

## ğŸš <span style="font-size:26px;">Audio Preprocessing</span>

All inputs are **.wav files**. For any `.mp3` data, automatic conversion is performed with **FFmpeg**:

```bash
for f in *.mp3; do ffmpeg -i "$f" "${f%.mp3}.wav"; done
```

### Extracted Features (Librosa):
- MFCCs  
- Chroma STFT  
- Spectral Centroid & Bandwidth  
- Zero-Crossing Rate  
- Root Mean Square Energy  

---

## ğŸ§  <span style="font-size:26px;">Model Training</span>

| **Model File** | **Dataset** | **Architecture** | **Epochs** | **Input Features** |
|----------------|--------------|------------------|-------------|--------------------|
| `emotion_model_ravdess.h5` | RAVDESS | CNN | 50 | MFCC + Chroma |
| `emotion_model_crema.h5` | CREMA-D | BiLSTM | 60 | MFCC |
| `emotion_model_emodb.h5` | EMO-DB | CNN + BiLSTM | 40 | MFCC + Spectral |

ğŸ“Š Confusion matrices (`*_confusion_matrix.png`) visualize accuracy for each trained model.

---

## ğŸ—‚ï¸ <span style="font-size:26px;">Project Structure</span>

```
emotion-tracker/
â”‚
â”œâ”€â”€ app.py                          # Streamlit inference app
â”œâ”€â”€ main.py                         # Feature extraction & model training
â”œâ”€â”€ data.ipynb                      # EDA & preprocessing notebook
â”œâ”€â”€ Dockerfile                      # Docker build file
â”œâ”€â”€ requirements.txt                # Dependencies list
â”‚
â”œâ”€â”€ ravdess_features.pkl
â”œâ”€â”€ crema_features.pkl
â”œâ”€â”€ emodb_features.pkl
â”‚
â”œâ”€â”€ emotion_model_ravdess.h5
â”œâ”€â”€ emotion_model_crema.h5
â”œâ”€â”€ emotion_model_emodb.h5
â”‚
â”œâ”€â”€ *_confusion_matrix.png          # Model evaluation plots
â”œâ”€â”€ .dockerignore
â”œâ”€â”€ .gitignore
â””â”€â”€ zipfolder/                      # Archived artifacts
```

---

## ğŸ’» <span style="font-size:26px;">Deployment Instructions</span>

### ğŸ§© Local Run
```bash
docker build -t emotion-recognition-app .
docker run -p 8501:8501 emotion-recognition-app
```
â¡ï¸ Access locally at: **[http://localhost:8501](http://localhost:8501)**

---

### â˜ï¸ Deploy to Google Cloud Run
```bash
docker tag emotion-recognition-app gcr.io/global-tine-477418-b7/emotion-recognition-app:v1
gcloud auth configure-docker
docker push gcr.io/global-tine-477418-b7/emotion-recognition-app:v1

gcloud run deploy emotion-recognition-app   --image gcr.io/global-tine-477418-b7/emotion-recognition-app:v1   --platform managed   --region asia-south1   --allow-unauthenticated   --memory 2Gi   --port 8501
```

ğŸŒ  Cloud URL:  
`https://emotion-recognition-app-684612753531.asia-south1.run.app/`
## ğŸš€ Live Demo

My deployed app is available here:

ğŸ”— [https://emotion-recognition-app-684612753531.asia-south1.run.app](https://emotion-recognition-app-684612753531.asia-south1.run.app)

---

## âš™ï¸ <span style="font-size:26px;">GPU Configuration</span>

- Framework: **TensorFlow-GPU (CUDA 11.x + cuDNN)**  
- GPU usage monitored via:
  ```bash
  nvidia-smi
  ```
- Docker runs on CPU in production; GPU used during training locally.

---



## ğŸ”® <span style="font-size:26px;">Future Enhancements</span>

- ğŸ¤ Add microphone-based live emotion detection  
- ğŸŒ Expand multilingual support (Hindi)  
- âš™ï¸ Add FastAPI RESTful API endpoint  
- ğŸ” Implement CI/CD with GitHub Actions  
- ğŸ§  Experiment with Transformer-based models (Wav2Vec2, HuBERT)

---


---
